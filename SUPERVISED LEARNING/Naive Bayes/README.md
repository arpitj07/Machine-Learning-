
# Naive Bayes Algorithm

“Naive Bayes is a family of probabilistic algorithms that take advantage of probability theory and Bayes’ Theorem to predict the category of a sample (like a piece of news or a customer review). They are probabilistic, which means that they calculate the probability of each category for a given sample, and then output the category with the highest one.” [[**HERE**](https://qr.ae/TWhYrq)]


  - **Advantages** [[Here](https://qr.ae/TWhYAt)]
    - Very simple, easy to implement and fast.
    - If the NB conditional independence assumption holds, then it will converge quicker than discriminative models like logistic                 regression.
    - Even if the NB assumption doesn’t hold, it works great in practice.
    - Need less training data.
    - **Highly scalable**. It scales linearly with the number of predictors and data points.
    - It can be used for both binary and mult-iclass classification problems.
    - Handles continuous and discrete data.
    - Not sensitive to irrelevant features.


  - **Limitations** [[Here]()]
    - 


  - **Applications** [[Here](https://qr.ae/TWhYrq)]
    - Categorising News
    - Email Spam Detection
    - Sentiment Analysis
    - Medical Diagnosis
    - Weather Prediction

  - **OTHER USEFUL LINKS**
    - [When and why is a naive Bayes classifier a better/worse choice than a random forest classifier?](https://qr.ae/TWhYF7)
    - [What-are-the-advantages-of-using-a-naive-Bayes-for-classification](https://qr.ae/TWhYFI)
    - [What types of data sets are appropriate for Naive Bayes?](https://qr.ae/TWhYFC)
    - [When should I use Naive Bayes classifier over neural networks?](https://qr.ae/TWhYFO)
